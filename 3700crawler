#!/usr/bin/env python3

import argparse
import socket
import ssl
from html.parser import HTMLParser
from urllib.parse import urlparse
import sys

DEFAULT_SERVER = "www.3700.network"
DEFAULT_PORT = 443

visited_links = []
link_set = set({}) # set
flags = []

class MyHTMLParser(HTMLParser):
    def handle_starttag(self, tag, attrs):
        if (tag == 'a'):
            link = attrs[0][1]
            if (link not in visited_links and link.startswith('/fakebook/')):
                link_set.add(attrs[0][1])   # if a duplicate link is added, it does not get added to the set

    def handle_endtag(self, tag):
        # print("Encountered an end tag :", tag)
        pass

    def handle_data(self, data):
        # print("Encountered some data  :", data)
        if (data.startswith("FLAG:")):
            flags.append(data)
            print("We found a flag!")
            print(data)

class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password
        self.csrf_token = None
        self.session_id = None
        self.socket = None

    def init_connection(self):
        # creating + wrapping the socket
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)
        context.load_default_certs()
        mysocket = context.wrap_socket(sock, server_hostname=self.server)
        # establishing socket connection with server
        mysocket.connect((self.server, self.port))
        
        self.socket = mysocket
        

    def login(self):
        # GET the login page
        request = "GET /accounts/login/?next=/fakebook/ HTTP/1.0\r\nHost: www.3700.network\r\n\r\n"
        response = self.send_request(request)
        self.get_cookies(response)

        # enter login credentials
        login_credentials = f"username={self.username}&password={self.password}&csrfmiddlewaretoken={self.csrf_token}&next=/fakebook/"
        login_cred_length = len(login_credentials)

        # POST the login information
        request = f"POST /accounts/login/ HTTP/1.0\r\nHost: www.3700.network\r\nContent-Type: application/x-www-form-urlencoded\r\nCookie: csrftoken={self.csrf_token}; sessionid={self.session_id}\r\nContent-Length: {login_cred_length}\r\n\r\n{login_credentials}"
        response = self.send_request(request)
        self.get_cookies(response)

        return response
        

    def get_cookies(self, response):
        # print(response)

        # obtaining the csrf token
        [csrf_cookie] = list(filter(lambda x: x.startswith("csrftoken="), response["cookies"]))
        self.csrf_token = csrf_cookie.split("; ")[0].split("=")[1]

        # obtaining the session id
        [session_id_cookie] = list(filter(lambda x: x.startswith("sessionid="), response["cookies"]))
        self.session_id = session_id_cookie.split("; ")[0].split("=")[1]
		
		
    def send_request(self, request):
        self.init_connection()

        self.socket.send(request.encode('ascii'))

        # receiving data
        response = b''
        while True:
            data = self.socket.recv(100)
            if len(data) == 0:
                break
            else:
                response += data
        # print("Response:\n%s" % response.decode('ascii'))
        response_data = response.decode('utf-8')

        self.socket.close()

        return self.parse_http_response(response_data)
    
    def parse_http_response(self, response_data):
        # Remove trailing whitespace and isolate the respone (separate it from HTML content)
        data = response_data.strip().split("\r\n\r\n")

        response = {}
        headers = data[0].split("\r\n")

        # extracts status code
        response["status_code"] = int(headers[0].split(" ")[1])

        response["header_fields"] = {}
        response["cookies"] = []
        for header in headers[1:]:
            (field, content) = header.split(": ")
            if field == "set-cookie":
                response["cookies"].append(content)
            else:
                response["header_fields"][field] = content

        response["body"] = data[-1] if data else None

        return response
    
    def scrape_page(q):
        pass

    def run(self):
        response = self.login()

        request = f"GET {response['header_fields']['location']} HTTP/1.0\r\nHost: www.3700.network\r\nCookie: csrftoken={self.csrf_token}; sessionid={self.session_id}\r\n\r\n"
        response = self.send_request(request)

        parser = MyHTMLParser()
        parser.feed(response["body"])

        # some_links = list(filter(lambda x: x.startswith("/fakebook/"), link_queue))

        # print(link_queue)

        while (len(link_set) > 0 and len(flags) < 1):
            current_link = link_set.pop()
            visited_links.append(current_link)
            request = f'GET {current_link} HTTP/1.0\r\nHost: {DEFAULT_SERVER}\r\nCookie: csrftoken={self.csrf_token}; sessionid={self.session_id}\r\n\r\n'
            # print("Request: %s" % request)
            response = self.send_request(request)

            parser.feed(response["body"])
            print(("# Links Visited: %s" % len(visited_links)).ljust(30, " ") + 
                  ("# Links To Visit: %s" % len(link_set)).ljust(30, " ") + 
                  ("Visiting: %s" % current_link))

        for flag in flags:
            sys.stdout.write(flag.replace("FLAG: ", ""))
        # print(flags)




        



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()